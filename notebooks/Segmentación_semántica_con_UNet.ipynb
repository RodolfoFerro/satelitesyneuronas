{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodolfoFerro/satelitesyneuronas/blob/main/notebooks/Segmentaci%C3%B3n_sem%C3%A1ntica_con_UNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6jO_1gISKxk"
      },
      "source": [
        "# Segmentación semántica con UNet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trabajos relacionados y avances recientes\n",
        "\n",
        "\n",
        "Ha habido varios trabajos de investigación y avances recientes que han contribuido al desarrollo de nuevas arquitecturas, técnicas de entrenamiento mejoradas y aplicaciones emergentes.\n",
        "\n",
        "- **UNet:** Es ampliamente utilizada en el campo de la segmentación de imágenes, pero también se ha aplicado con éxito en tareas de denoising.\n",
        "- **Variational Autoencoders (VAEs):** Los VAEs son una variante de los autoencoders que se utilizan para el aprendizaje de distribuciones latentes. Han demostrado ser efectivos en el denoising de imágenes al aprender representaciones latentes que siguen una distribución probabilística, lo que permite una generación más controlada y realista de imágenes limpias.\n",
        "- **GANs y Autoencoders Generativos (GANs-AE):** La combinación de las GANs y los AE ha llevado al desarrollo de los GANs-AE. Estos modelos aprovechan la capacidad de los GANs para generar imágenes realistas y los autoencoders para aprender representaciones latentes eficientes. Los GANs-AE han demostrado ser efectivos en el denoising y la generación de imágenes de alta calidad.\n",
        "\n",
        "\n",
        "#### **Tareas en el campo de la visión artificial**\n",
        "\n",
        "1. **Clasificación de imágenes:** La tarea de clasificación de imágenes implica asignar una etiqueta o categoría a una imagen de entrada. Esto implica entrenar un modelo para reconocer y distinguir diferentes objetos, personas o escenas en una imagen.\n",
        "2. **Detección de objetos:** La detección de objetos implica localizar y clasificar múltiples objetos en una imagen. El objetivo es detectar la presencia y la ubicación de objetos específicos en una escena, a menudo utilizando cuadros delimitadores para delinear las regiones donde se encuentran los objetos.\n",
        "3. **_Denoising_ o reconstrucción de imágenes:** Consiste en eliminar o reducir el ruido presente en una imagen, obteniendo una versión más limpia y clara. Esta tarea es relevante en áreas como la fotografía, la medicina y la seguridad..\n",
        "4. **Segmentación semántica:** La segmentación semántica implica asignar una etiqueta a cada píxel de una imagen para identificar y delimitar las diferentes regiones o objetos presentes. El objetivo es comprender la estructura y el contenido de una imagen a nivel de píxel.\n",
        "5. **Detección de rostros:** La detección de rostros es una tarea específica de la visión artificial que implica detectar y localizar los rostros en una imagen. Es ampliamente utilizado en aplicaciones de reconocimiento facial, análisis de emociones y sistemas de seguridad.\n",
        "6. **Reconocimiento y verificación facial:** El reconocimiento facial se refiere a la tarea de identificar y reconocer a una persona específica a partir de una imagen o secuencia de imágenes. La verificación facial se enfoca en verificar si una imagen de rostro coincide con una identidad específica.\n",
        "7. **Estimación de pose:** La estimación de pose se refiere a la tarea de determinar la posición y orientación de un objeto o persona en una imagen. Esto implica detectar y rastrear las articulaciones o puntos clave en una imagen para comprender la postura y el movimiento.\n",
        "8. **Estimación de profundidad:** La estimación de profundidad implica inferir la información de la distancia o la profundidad de los objetos en una imagen. Es útil en aplicaciones de realidad virtual, conducción autónoma y sistemas de navegación.\n",
        "9. **Super-resolución:** La super-resolución se refiere a aumentar la resolución o la calidad de una imagen de baja resolución. El objetivo es generar una versión de alta resolución que capture más detalles y claridad.\n"
      ],
      "metadata": {
        "id": "JP3NGkSg6fO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lecturas recomendadas:**\n",
        "\n",
        "- [Convolution, Padding, Stride, and Pooling in CNN](https://medium.com/analytics-vidhya/convolution-padding-stride-and-pooling-in-cnn-13dc1f3ada26)\n",
        "- [Why do we need conv2d_transpose?\n",
        "](https://medium.com/@vaibhavshukla182/why-do-we-need-conv2d-transpose-2534cd2a4d98)\n",
        "- [Deconvolution](https://vincmazet.github.io/bip/restoration/deconvolution.html)\n",
        "- [Image Segmentation using deconvolution layer in Tensorflow](https://cv-tricks.com/image-segmentation/transpose-convolution-in-tensorflow/)"
      ],
      "metadata": {
        "id": "l2OVuOuwHqv0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_BXRzLrBEdd"
      },
      "source": [
        "### **Clonamos el repositorio**\n",
        "\n",
        "Comenzaremos clonando el repositorio y asignando a la carpeta como la raíz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p7Tqyz-zfti"
      },
      "source": [
        "!git clone https://github.com/RodolfoFerro/satelitesyneuronas.git\n",
        "%cd satelitesyneuronas\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8IkF4N2DJV6"
      },
      "source": [
        "La estructura del código fuente es como sigue:\n",
        "- `tools/unet/model.py` - Contiene la implementación del U-Net.\n",
        "- `tools/data.py` - Contiene funciones de utilería para carga de datos.\n",
        "- `tools/image.py` - Coniene funciones de utilería para cargar imágenes y mostrar los resultados de las inferencias.\n",
        "- `main.py` - Contiene una sencilla implementación de este cuaderno en un script de Python para entrenar el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA1wiuc01lQd"
      },
      "source": [
        "### **Cargamos los datos**\n",
        "\n",
        "A continuación procedemos a importar algunas bibliotecas y el código base del modelo.\n",
        "\n",
        "Haremos uso de alunas funciones que permiten cargar datos que encuentras en el folder `data`.\n",
        "\n",
        "Comenzaremos importando las funciones de los módulos a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kdal-XjznDC"
      },
      "source": [
        "from tools.data import train_generator\n",
        "from tools.data import test_generator\n",
        "from tools.data import save_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLq_Gu9hDxX4"
      },
      "source": [
        "Procedemos a crear un diccionario de configuración para cargar datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGXyhvn7bW0h"
      },
      "source": [
        "data_gen_args = dict(\n",
        "    rotation_range=0.2,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    shear_range=0.05,\n",
        "    zoom_range=0.05,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_gen = train_generator(\n",
        "    2, 'assets/data/membrane/train',\n",
        "    'image', 'label',\n",
        "    data_gen_args,\n",
        "    save_to_dir=None\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Creamos el modelo**\n",
        "\n",
        "Ahora, procederemos a crear el modelo. Para ello, dos opciones serán previstas.\n",
        "\n",
        "**OPCIÓN A:** Creamos nuestro propio U-Net con nuestras propias características, basándonos en la propuesta original:\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.08.00_PM_rpNArED.png\" width=\"60%\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "K87oTtdQhdui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def unet(pretrained_weights=None, input_size=(256, 256, 1)):\n",
        "    \"\"\"U-Net model constructor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pretrained_weights : str\n",
        "        Path to pretrained weights.\n",
        "    input_size : tuple\n",
        "        Spatial size of the expected input image.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Convolution chain #1\n",
        "    # conv_1 = ...\n",
        "\n",
        "    # Continua aquí con tu propia implementación..."
      ],
      "metadata": {
        "id": "QAH4Q0TxiSnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttz1pS1SD5su"
      },
      "source": [
        "**OPCIÓN B:** Creamos una instancia del modelo ya implementado y entrenamos con los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKZb95kubdUf"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "from tools.unet.model import unet\n",
        "\n",
        "# Replicable experiments\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = unet()\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'unet_membrane.keras',\n",
        "    monitor='loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=300,\n",
        "    epochs=5,\n",
        "    callbacks=[model_checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "Xz0G99UGYndV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import numpy as np\n",
        "\n",
        "losses = history.history['loss']\n",
        "eje_x = np.arange(len(losses))\n",
        "\n",
        "fig = px.line(\n",
        "    x=eje_x,\n",
        "    y=losses,\n",
        "    title='Historia de entrenamiento',\n",
        "    labels=dict(x='Épocas', y='Error')\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "jodtKIlZMOSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edeZ8WvsD89n"
      },
      "source": [
        "**Hasta este punto deberías haber entrenado exitosamente un U-Net con algunas imágenes médicas.**\n",
        "\n",
        "Una vez entrenado el modelo, podemos realizar pruebas de inferencia con el conjunto de pruebas que se encuentra en la misma carpeta de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i_ECOlzefxU"
      },
      "source": [
        "test_gen = test_generator('assets/data/membrane/test')\n",
        "results = model.predict(test_gen, verbose=True)\n",
        "save_results('data/results', results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2-wE6B7jFj4"
      },
      "source": [
        "\n",
        "### **Resultados gráficos**\n",
        "\n",
        "El código base provee algunas funciones para cargar, inferir y crear máscaras de los resultados al trabajar sobre algunas imágenes.\n",
        "\n",
        "Procedemos a importar las funciones del módulo de imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU8bP3t25TDm"
      },
      "source": [
        "from tools.image import load_test_image\n",
        "from tools.image import inference_over_image\n",
        "from tools.image import create_mask\n",
        "from tools.image import overlay_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRE71y6dEchq"
      },
      "source": [
        "Cargamos una imagen del directorio de prueba, especificando con un número entero el índice de alguna de las 30 imágenes (`[0, 29]`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzjLv6Rd8KuB"
      },
      "source": [
        "img = load_test_image(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUkAB_jOEpGz"
      },
      "source": [
        "Usamos el modelo previamente entrenado para inferir sobre la imagen previamente cargada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugcpTWtHp_rZ"
      },
      "source": [
        "out = inference_over_image(model, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd4rdm-4Ey2H"
      },
      "source": [
        "Creamos una máscara a partir de la inferencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNylyGmZ0xEw"
      },
      "source": [
        "mask = create_mask(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEjSK__TE1j8"
      },
      "source": [
        "Sobreponemos la máscara en la imágen original para validar el resulatdo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9Q4oxihCgSq"
      },
      "source": [
        "res = overlay_mask(img, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsmVIiyTE51p"
      },
      "source": [
        "**¡Felicidades! Has utilizado exitosamente tu modelo entrenado sobre algunas imágenes médicas.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reto:** Yo me he encargado de enfocarme en utilizar las detecciones para la identificación de pared celular, sin embargo, puedes modificar o crear tus propias funciones para la detección celular completa.\n",
        "\n",
        "Por otro lado, no debes limitarte a ello, sino que puedes crear o cargar tu propio conjunto de datos para segmentar otro tipo de elementos, como las mitocondrias (echa un vistazo al [Electron Microscopy Dataset](https://www.epfl.ch/labs/cvlab/data/data-em/))."
      ],
      "metadata": {
        "id": "c-g406-ALN_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> Contenido curado por **Rodolfo Ferro**. Contacto: [ferro@cimat.mx](ferro@cimat.mx) <br>\n",
        "[**Clubes de Ciencia México**](https://clubesdeciencia.mx/), 2025."
      ],
      "metadata": {
        "id": "hSdbQU3e6-Ky"
      }
    }
  ]
}